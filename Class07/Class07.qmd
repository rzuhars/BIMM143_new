---
title: "Class 07- Machine Learning"
author: "Renee Zuhars (PID: A17329856)"
format: pdf
toc: TRUE
---

Today we will explore unsupervised machine learning methods starting with clustering and dimensionality reduction.

## Clustering

To start let's make up some data to cluster where we know what the answer should be. The `rnorm()` function will help us here. 

```{r}
hist( rnorm(10000, mean=3) )
```

Return 30 numbers (a vector of 30 elements) centered on -3. Then do the same with positive 3. 

```{r}
tmp <- c( rnorm(30, mean=-3), 
        rnorm(30, mean=3) )

x <- cbind(x=tmp, y=rev(tmp))

x
```

Make a plot of `x`. 

```{r}
plot(x)
```

### K-means

The main function in "base" R for K-means clustering is called `kmeans()`. 

```{r}
km <- kmeans(x, centers=2)
km
```

The `kmeans()` function return "list" with 9 components. 
```{r}
attributes(km)
```

> Q. How many points are in each cluster?

```{r}
km$size
```

> Q. Cluster assignment/membership vector?

```{r}
km$cluster
```

> Q. Cluster centers?

```{r}
km$centers
```

> Q. Make a plot of our `kmeans()` results showing cluster assignment using different colors for each cluster/group of points and cluster centers. 

```{r}
plot(x, col=km$cluster) #color by cluster
points(km$centers, col="green", pch=15, cex=2)
```

> Q. Run `kmeans` again on `x` and create 4 groups/clusters. Plot the same result figure as above.

```{r}
km4 <- kmeans(x, centers=4)
plot(x, col=km4$cluster)
points(km4$centers, col="violet", pch=15)

```

> **key point**: K means clustering is super popular but can be misused. One big limitation is that it can impose a clustering pattern on your data even if clear natural grouping doesn't exist- i.e. it does what you tell it to do in terms of `centers`. 

### Hierarchical Clustering

The main function in "base" R for hierarchical clustering is called `hclust()`

You can't just pass our dataset as is into `hclust()` you must give "distance matrix" as input. We can get this from the `dist()` function in R. 

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

The results of `hclust()` don't have a useful `print()` method but do have a special `plot()` method.

```{r}
plot(hc)
abline(h=8, col='red')
```

To get our main cluster assignment (membership vector) we need to "cut" the tree at the big goalposts

```{r}
grps <- (cutree(hc, h=8))
grps
```

```{r}
table(grps)
```

```{r}
plot(x, col=grps)
```

Hierarchical Clustering is distinct in that the dendrogram (tree figure) can reveal the potential grouping in your data

## Principal Component Analysis (PCA)

PCA is a common and highly useful dimensionality reduction technique used in many fields- particularly bioinformatics.

Here we will analyze some data from the UK on food consumption.

### Data import

```{r}
url <- 'https://tinyurl.com/UK-foods'
x <- read.csv(url)
head(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1] #destructive, removes each time
head(x)
```

```{r}
x <- read.csv(url, row.names = 1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

One conventional plot that can be useful is called a "pairs" plot (plot of all pairwise combinations of countries next to each other)

```{r}
pairs(x, col=rainbow(10), pch=16)
```
This pairwise plot produces 12 graphs directly comparing each country to each other country. 

### PCA to the rescue

The main function in base R for PCA is called `prcomp()`.

```{r}
pca <- prcomp( t(x) )
summary(pca)
```
PC1 vs PC2 results in 96.5% coverage (see cumulative proportion)

The `prcomp()` function returns a list object of our results with

```{r}
attributes(pca)
```

The two main "results" in here are `pca$x` and `pca$rotation`. The first of these (`pca$x`) contains the scores of the data on the new PC axis- we use these to make our "PCA plot". 

```{r}
pca$x
```

> Q. Make a plot of pca$x with PC1 vs PC2

```{r}
library(ggplot2)

ggplot(pca$x) + 
  aes(PC1, PC2, label=rownames(pca$x)) +
  geom_point() +
  geom_label()

```
This plot shows the distribution spread of the four countries. 

The second major result is contained in the `pca$rotation` object or component. Let's plot this to see what PCA is picking up...

```{r}
pca$rotation

```

```{r}
ggplot(pca$rotation) +
  aes(PC1, rownames(pca$rotation)) +
  geom_col()
```
This plot compares the PC1 to our original dataset. 
